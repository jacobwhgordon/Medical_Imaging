{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying FNC8 to our images and masks.\n",
    "\n",
    "#This is a failed attempt to write my own pyTorch FCN8 implementation and run it on my local CPU, I did not have enough ram.\n",
    "#I will leave it here for completeness, but note that it relies on libraries I am not uploading (kural_core and fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But first!  Import stuff.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from kural_core.models import *\n",
    "from kural_core.data_processing import *\n",
    "\n",
    "PATH = 'E:\\\\DicomData\\\\LUNG1\\\\NSCLC-Radiomics\\\\'\n",
    "\n",
    "# Stuff for dicom data\n",
    "import os\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "import nrrd\n",
    "\n",
    "\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.draw import polygon\n",
    "\n",
    "# My stuff\n",
    "import function as my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load the raw data. (for now load the first 10 patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNG1-001\n",
      "  09-18-2008-StudyID-69331\n",
      "    0-82046\n",
      "    0-95085\n",
      "LUNG1-002\n",
      "  01-01-2014-StudyID-85095\n",
      "    1-61228\n",
      "    1-63982\n",
      "LUNG1-003\n",
      "  01-01-2014-StudyID-34270\n",
      "    1-28595\n",
      "    1-65174\n",
      "LUNG1-004\n",
      "  09-24-2006-StudyID-27873\n",
      "    0-20785\n",
      "    0-68092\n",
      "LUNG1-005\n",
      "  01-01-2014-StudyID-93819\n",
      "    1-68747\n",
      "    1-85493\n",
      "LUNG1-006\n",
      "  01-01-2014-StudyID-99263\n",
      "    1-58122\n",
      "    1-69736\n",
      "LUNG1-007\n",
      "  05-28-2010-36277\n",
      "    0-67872\n",
      "LUNG1-008\n",
      "  01-01-2014-StudyID-52421\n",
      "    1-44475\n",
      "    1-77383\n",
      "LUNG1-009\n",
      "  01-01-2014-StudyID-54035\n",
      "    1-63984\n",
      "    1-24689\n",
      "LUNG1-010\n",
      "  01-01-2014-StudyID-54264\n",
      "    1-08510\n",
      "    1-83025\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "patients = [p for p in os.listdir(PATH)]\n",
    "for p in patients[0:10:1]:\n",
    "    print(p)\n",
    "    for s in os.listdir(PATH+p+\"\\\\\"):\n",
    "        dir2 = [d for d in os.listdir(PATH+p+\"\\\\\"+s+\"\\\\\")]        \n",
    "        print(\"  \" + s)\n",
    "        folder = \"\"\n",
    "        folderC = \"\"\n",
    "        if len(dir2) > 1:\n",
    "            #We need to check which folder has the slices vs the contours.  There is a bunch of slices, but 1 contour.\n",
    "            if (len(os.listdir(PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[1]))) == 1:\n",
    "                print(\"    \" + dir2[0])\n",
    "                print(\"    \" + dir2[1])\n",
    "                folder = PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[0]+\"\\\\\"\n",
    "                folderC = PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[1]+\"\\\\\"\n",
    "            else:\n",
    "                print(\"    \" + dir2[1])\n",
    "                print(\"    \" + dir2[0])\n",
    "                folder = PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[1]+\"\\\\\"\n",
    "                folderC = PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[0]+\"\\\\\"\n",
    "            contour = my.loadContour(folderC)\n",
    "        else: \n",
    "            print(\"    \" + dir2[0])\n",
    "            folder = PATH+p+\"\\\\\"+s+\"\\\\\"+dir2[0]+\"\\\\\"\n",
    "            contour = []\n",
    "        slices = my.loadScan(folder)\n",
    "        data.append([slices,contour])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now mask the stuff that is outside the lungs out of our slices  (this takes ~10 minutes for ~10 patients)\n",
    "# masked_data is a list of stacks of pixel arrays.\n",
    "\n",
    "mask_data = []\n",
    "for p in data:\n",
    "    mask_slices = []\n",
    "    for s in p[0]:\n",
    "        mask_slices.append(my.maskLung(s.pixel_array))\n",
    "    mask_data.append(mask_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No contours manually identified\n"
     ]
    }
   ],
   "source": [
    "#Now create/match all the contours representing tumors to the slices from the stacks\n",
    "\n",
    "tumors = []\n",
    "colors = []\n",
    "for patient in data:\n",
    "    tum,col = my.labelTumor(patient)\n",
    "    tumors.append(tum)\n",
    "    colors.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ab64af0f60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we need to create our datasets. we probably dont want to train on all of the slices, because thats a waste of time.\n",
    "# But if we focus on training on the slices near the centers of the contrours how will our algorthm learn what isnt\n",
    "# a Tumor?\n",
    "\n",
    "inputImages =\n",
    "plt.imshow(tumors[1][...,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop lists the slices with tumors and outputs them as a 2d array\n",
    "data_we_care_about = []\n",
    "for i,p in enumerate(tumors):\n",
    "    t_slices = []\n",
    "    for j in range(p.shape[-1]):\n",
    "        if p[...,j].any() != 0:\n",
    "            #print(i,j)\n",
    "            t_slices.append(j)\n",
    "    data_we_care_about.append(t_slices)\n",
    "    \n",
    "#now lets pull out the middle 50% of each of these tumors, to avoid training on edge effects.\n",
    "\n",
    "data_we_really_care_about = []\n",
    "for stuff in data_we_care_about:\n",
    "    n = len(stuff)\n",
    "    data_we_really_care_about.append(stuff[int(n/4):int(3*n/4):1])\n",
    "#print(data_we_really_care_about)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 3, 512, 512)\n",
      "(98, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "#format data for torch\n",
    "input_data = []\n",
    "input_mask = []\n",
    "for i,p in enumerate(mask_data):\n",
    "    for j,slices in enumerate(p):\n",
    "        if j in data_we_really_care_about[i]:\n",
    "            input_data.append([p[j-1],slices,p[j+1]])                    #3 'colors'\n",
    "            input_mask.append([tumors[i][...,j-1],tumors[i][...,j],tumors[i][...,j+1]])\n",
    "            \n",
    "            \n",
    "#convert to np array\n",
    "input_data = np.array(input_data,dtype='f')\n",
    "input_mask = np.array(input_mask,dtype='f')\n",
    "print(input_data.shape)\n",
    "print(input_mask.shape)\n",
    "\n",
    "data_file = \"data_file.npy\"\n",
    "mask_file = \"mask_file.npy\"\n",
    "#output out data and mask to a file\n",
    "np.save(data_file, input_data)\n",
    "np.save(mask_file, input_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data_file.npy\"\n",
    "mask_file = \"mask_file.npy\"\n",
    "#load our data and mask from a file\n",
    "input_data = np.load(data_file)\n",
    "input_mask = np.load(mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives us 98 sets of images to train on. (for now) aranged as a list of np arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BoundingLandmarksDataset(input_data,input_mask) ## see kural_core/data_processing.py\n",
    "\n",
    "train_idx, val_idx = RandomIndicesForKFoldValidation(len(ds), 0, K=5, rand_seed=42, nORp=1) ## see kural_core/data_processing.py\n",
    "\n",
    "bs = 4 # batch size\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(ds,batch_size=bs, sampler=SubsetSampler(train_idx, shuffle=True))\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(ds,batch_size=5*bs, sampler=SubsetSampler(val_idx))\n",
    "\n",
    "#model = BoundingPointsModel().cpu()\n",
    "model = FCN8Model().cpu() ## see kural_core/models.py\n",
    "\n",
    "lr_start = 1e-3\n",
    "\n",
    "opt = optim.Adam(model.parameters(),lr=lr_start)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart number 1 out of 1\n",
      "Cycle: 0 ~ Remaining total: Unknown ~ Epoch: 0 ~ Remaining in cycle: 0s"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 4GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0d1b395d768f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## alter this as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## alter this as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\JupyterNotebooks\\Medical_Imaging\\kural_core\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 4GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "restarts = 1\n",
    "lr_end = lr_start\n",
    "for restart in range(restarts):\n",
    "    cycles = 1\n",
    "    epochs = 20\n",
    "    print(f'restart number {restart+1} out of {restarts}')\n",
    "    time_list = []\n",
    "    epoch_time_list = None\n",
    "    cycle_print_str = 'Cycle: 0 ~ Remaining total: Unknown'\n",
    "    print(cycle_print_str,end='')\n",
    "    for cycle in range(cycles):\n",
    "        ts = time.clock()\n",
    "        if cycle>0: epochs = epochs*2\n",
    "        lr_start = lr_start*(0.999)**cycle\n",
    "        epoch_print_str = f' ~ Epoch: 0 ~ Remaining in cycle: {(epochs*np.mean(epoch_time_list) if epoch_time_list is not None else 0):.3g}s'\n",
    "        print(epoch_print_str, end='')\n",
    "        epoch_time_list = []\n",
    "        for epoch in range(epochs):\n",
    "            ets = time.clock()\n",
    "            opt.param_groups[0]['lr'] = lr_end + 0.5*(lr_start-lr_end)*(1+math.cos(cycle/cycles*np.pi))\n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "            for data in train_dl:\n",
    "                x_train = data[0].cpu() ## alter this as needed\n",
    "                opt.zero_grad()\n",
    "                y_pred = model(x_train)\n",
    "                loss = criterion(y_pred,data[1].view(-1,60).cpu()) ## alter this as needed\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                running_loss += loss.item()\n",
    "            train_loss.append(running_loss/len(train_dl))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in val_dl:\n",
    "                    x_train = data[0].cpu() ## alter this as needed\n",
    "                    y_pred = model(x_train)\n",
    "                    running_loss += criterion(y_pred,data[1].view(-1,60).cpu()).item() ## alter this as needed\n",
    "                val_loss.append(running_loss/len(val_dl))\n",
    "            epoch_time_list.append(time.clock()-ets)\n",
    "            if epoch_print_str: \n",
    "                for _ in range(len(epoch_print_str)):\n",
    "                    print('\\b',end='')\n",
    "            epoch_print_str = f' ~ Epoch: {epoch+1:2d}/{epochs} ~ Remaining in cycle: {(epochs-(epoch+1))*np.mean(epoch_time_list):.3g}s'\n",
    "            print(epoch_print_str,end='')\n",
    "        time_list.append(time.clock()-ts)\n",
    "        if cycle_print_str: \n",
    "            for _ in range(len(cycle_print_str)+len(epoch_print_str)):\n",
    "                print('\\b',end='')\n",
    "        remaining_time = (cycles-(cycle+1))*np.mean(time_list)\n",
    "        cycle_print_str = f'Cycle: {cycle+1:3d}/{cycles} ~ Remaining total: {int(remaining_time//60)}m {remaining_time%60:02.0f}s'\n",
    "        print(cycle_print_str,end='')\n",
    "    for _ in range(len(cycle_print_str)):\n",
    "                print('\\b',end='')\n",
    "    print(f'Spent {int(sum(time_list)//60)}m {sum(time_list)%60:2.0f}s doing {cycles*epochs} total steps for an average of {sum(time_list)/cycles/epochs:3.1f}s per step.')\n",
    "\n",
    "torch.save(model.state_dict(),'model_out.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
